- name: Hosts initialization
  tags: hosts initialization
  hosts: controls, workers
  become: true
  tasks:
    - name: Check host availability
      ansible.builtin.ping:

    - name: Add lines to /etc/hosts
      tags: setup hosts
      ansible.builtin.blockinfile:
        dest: /etc/hosts
        block: |
          192.168.56.10 haproxy-ext-1
          192.168.56.11 haproxy-ext-2
          192.168.56.20 haproxy-int
          192.168.56.21 haproxy-int-1
          192.168.56.22 haproxy-int-2
          192.168.56.31 control-1
          192.168.56.32 control-2
          192.168.56.33 control-3
          192.168.56.41 node-1
          192.168.56.42 node-2
          192.168.56.43 node-3
          192.168.56.51 database-1
          192.168.56.51 database-2
          192.168.56.51 database-3
        state: present

    - name: Disable ufw service
      ansible.builtin.service:
        name: ufw
        state: stopped
        enabled: false

    - name: Set timezone to Europe/Moscow
      community.general.timezone:
        name: Europe/Moscow

    - name: Uninstall 'ntp' package
      ansible.builtin.apt:
        name: ntp
        state: absent

    - name: Turning "set-ntp" on
      command: /usr/bin/timedatectl set-ntp on

# ==========================================

- name: Install kubernetes prerequirements
  tags: install kubernetes prerequirements
  hosts: controls, workers
  become: true
  tasks:
    - name: Check host availability
      ansible.builtin.ping:

    - name: Disable SWAP since kubernetes can't work with swap enabled (1/2)
      command: swapoff -a

    - name: Disable SWAP in fstab since kubernetes can't work with swap enabled (2/2)
      replace:
        path: /etc/fstab
        regexp: '^([^#].*?\sswap\s+sw\s+.*)$'
        replace: '# \1'

    - name: Load modules "overlay" and "br_netfilter"
      community.general.modprobe:
        name: "{{ item }}"
        state: present
        persistent: present
      with_items:
        - overlay
        - br_netfilter

    - name: setup forward packages across routers
      ansible.posix.sysctl:
        name: "{{ item }}"
        value: "1"
        state: present
      with_items:
        - net.ipv4.conf.all.forwarding
        - net.bridge.bridge-nf-call-ip6tables
        - net.bridge.bridge-nf-call-iptables

    - name: Add Docker's official GPG key
      ansible.builtin.apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        keyring: /etc/apt/keyrings/docker.gpg
        state: present

    - name: Add Docker repository
      ansible.builtin.apt_repository:
        repo: deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu {{ ansible_lsb.codename }} stable
        filename: docker
        state: present

    - name: Install additional software
      tags: install software
      ansible.builtin.apt:
        name:
          - ca-certificates
          - apt-transport-https
          - curl
          - gpg
          - gnupg2
          - software-properties-common
          - containerd.io
          - nfs-common
          - postgresql-client
        state: present
        update_cache: yes

    - name: Add containerd configuration directory
      ansible.builtin.file:
        path: /etc/containerd
        state: directory

    - name: Generate the default config.toml file
      tags: generate config.toml
      shell: "containerd config default > /etc/containerd/config.toml"

    - name: Set SystemdGroup=true in the config.toml
      tags: set SystemdGroup
      ansible.builtin.lineinfile:
        path: /etc/containerd/config.toml
        regexp: '(\s*)SystemdCgroup = false'
        line: '\1SystemdCgroup = true'
        backrefs: yes
      notify: restart containerd

    - name: Install crictl
      tags: install crictl
      ansible.builtin.unarchive:
        src: https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.32.0/crictl-v1.32.0-linux-amd64.tar.gz
        dest: /usr/local/bin
        remote_src: yes

    - name: Configure critctl
      tags: configure critctl
      ansible.builtin.blockinfile:
        dest: /etc/crictl.yaml
        block: |
          runtime-endpoint: unix:///run/containerd/containerd.sock
          image-endpoint: unix:///run/containerd/containerd.sock
          timeout: 2
          debug: true # <- if you don't want to see debug info you can set this to false
          pull-image-on-create: false
        create: true

    - name: Add Kubernetes's official GPG key
      tags: add kub rep key
      ansible.builtin.apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key
        keyring: /etc/apt/keyrings/kubernetes-apt-keyring.gpg
        state: present

    - name: Add Kubernetes's repository
      tags: add kub repo
      ansible.builtin.apt_repository:
        repo: deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
        filename: kubernetes
        state: present

    - name: Install kubelet kubeadm kubectl
      tags: install kubs app
      ansible.builtin.apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes

    - name: Apt hold kubelet, kubeadm, kubectl
      tags: hold kub services
      ansible.builtin.dpkg_selections:
        name: "{{ item }}"
        selection: hold
      with_items:
        - kubelet
        - kubeadm
        - kubectl

    - name: Enable kubelet service
      tags: enable kubelet
      ansible.builtin.service:
        name: kubelet
        enabled: yes

    - name: Add Helms's official GPG key
      tags: install helm
      ansible.builtin.apt_key:
        url: https://baltocdn.com/helm/signing.asc
        keyring: /etc/apt/keyrings/helm.gpg
        state: present
      when: inventory_hostname in groups["controls"]

    - name: Add Helms repository
      tags: install helm
      ansible.builtin.apt_repository:
        repo: deb [arch=amd64 signed-by=/etc/apt/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main
        filename: helm
        state: present
      when: inventory_hostname in groups["controls"]

    - name: Install Helm
      tags: install helm
      ansible.builtin.apt:
        name: helm
        state: present
        update_cache: yes
      when: inventory_hostname in groups["controls"]

  handlers:
    - name: restart containerd
      tags: restart containerd
      ansible.builtin.service:
        name: containerd
        state: restarted

# ========================================

- name: Set up control-1
  tags: set up control-1
  hosts: control-1
  become: true
  tasks:
    - name: Check host availability
      ansible.builtin.ping:

    - name: Initialize the kubernetes control plane
      tags: initialize control plane
      command: kubeadm init --control-plane-endpoint "haproxy-int:6443" --upload-certs --apiserver-advertise-address={{ansible_eth1.ipv4.address}} --pod-network-cidr=10.95.0.0/16 --service-cidr=10.96.0.0/16
      register: kubeadm_output
      ignore_errors: true

    - name: Generate new token again
      tags: gen token
      command: kubeadm token create --print-join-command
      register: gen_token_output

    - name: Set workers_join_command
      tags: gen token
      set_fact:
        workers_join_command: "{{gen_token_output.stdout}}"

    - name: Show workers_join_line
      tags: gen token
      debug: msg="{{workers_join_command}}"

    - name: Upload cluster certs again
      tags: upload certs
      command: kubeadm init phase upload-certs --upload-certs
      register: cluster_certs

    - name: Set masters_join_command
      tags: upload certs
      set_fact:
        masters_join_command: "{{workers_join_command}}--control-plane --certificate-key {{cluster_certs.stdout_lines[2]}}"

    - name: show masters_join_command
      tags: upload certs
      debug: msg="{{masters_join_command}}"

    - name: Create .kube directory
      tags: copy admin.conf on master1
      ansible.builtin.file:
        path: $HOME/.kube
        state: directory
        mode: 0755

    - name: Copy admin.conf $HOME/.kube
      tags: copy admin.conf on master1
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: $HOME/.kube/config
        remote_src: yes

    - name: wait 60 seconds before add other controls
      ansible.builtin.wait_for:
        timeout: 60

# ==========================================

- name: Setup control-2, control-3
  tags: setup control-2 control-3
  hosts: control-2,control-3
  become: true
  tasks:
    - name: Check host availability
      ansible.builtin.ping:

    - name: Join control-2 and control-3 to control node cluster
      tags: join to control cluster
      command: "{{ hostvars['control-1']['masters_join_command'] }} --apiserver-advertise-address={{ansible_eth1.ipv4.address}}"
      when: hostvars['control-1']['masters_join_command'] is defined

    - name: Create .kube directory
      tags: copy admin.conf
      ansible.builtin.file:
        path: $HOME/.kube
        state: directory
        mode: 0755

    - name: Copy admin.conf $HOME/.kube
      tags: copy admin.conf
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: $HOME/.kube/config
        remote_src: yes

# ==========================================

- name: Setup workers
  tags: setup workers
  hosts: node-1,node-2,node-3
  become: true
  tasks:
    - name: Check host availability
      ansible.builtin.ping:

    - name: Join workers to kubernetes cluster
      tags: join workers to cluster
      command: "{{ hostvars['control-1']['workers_join_command'] }}"
      when: hostvars['control-1']['workers_join_command'] is defined

# ==========================================
#! здесь надо бы включить direct mode, что не было накладных расходов на ip-ip туннели (по умолчанию режим работы calico ip-ip. но так как у нас ноды
# в одной L2 сети, то нужно переключать на direct)
- name: Install CNI Calico
  tags: install cni calico
  hosts: control-1
  become: true
  tasks:
    - name: Add the Calico helm repo
      tags: add the calico helm repo
      command: helm repo add projectcalico https://docs.tigera.io/calico/charts

    - name: Create the tigera-operator namespace
      tags: create the tigera-operator namespace
      command: kubectl create namespace tigera-operator

    - name: Install the Tigera operator and custom resource definitions
      tags: install the Tigera operator
      command: helm install calico projectcalico/tigera-operator --version v3.29.2 --namespace tigera-operator

    # чтобы управлять с удалённой машины с помощью calicoctl нужно еще добавить конфиг. с control нод и так будет работать
    - name: install calicoctl
      tags: install calicoctl
      ansible.builtin.get_url:
        url: https://github.com/projectcalico/calico/releases/download/v3.29.2/calicoctl-linux-amd64
        dest: /usr/local/bin/calicoctl
        mode: "0755"

# ==========================================
# coredns приземляется почему-то на одну control ноду после первичной установки (и не обязательно на первую, на которой инициализирован кластер). Рестартим
# чтобы раскидать на две ноды

- name: Restart CoreDNS
  tags: restart coredns
  hosts: control-1
  become: true
  tasks:
    - name: Restart CoreDNS for distributing between control nodes
      command: kubectl -n kube-system rollout restart deployment coredns

# ==========================================

- name: Install Kubernetes dashboard
  tags: install kubernetes dashboard
  hosts: control-1
  become: true
  tasks:
    - name: Setup Kubernetes Dashboard
      command: "{{item}}"
      with_items:
        - helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
        - helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard

    #- name: Copy admin-user.conf and admin-user-binding.conf
    #  tags: copy admin conf
    #  copy:
    #    src: "{{item}}"
    #    dest: $HOME
    #  with_items:
    #    - templates/kubernetes/manifests/admin-user-service-account.yaml
    #    - templates/kubernetes/manifests/admin-user-cr-binding.yaml
    #    - templates/kubernetes/manifests/admin-user-secret.yaml

    - name: create admin-user
      tags: create admin-user
      command: "{{item}}"
      with_items:
        - kubectl create -f /vagrant/kubernetes/manifests/admin-user-service-account.yaml
        - kubectl create -f /vagrant/kubernetes/manifests/admin-user-cr-binding.yaml
        - kubectl create -f /vagrant/kubernetes/manifests/admin-user-secret.yaml

    - name: Get long-live token for admin-user
      tags: get long-live token
      shell: 'kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath="{.data.token}" | base64 -d'
      register: lltoken

    - name: Show long-live token
      tags: get long-live token
      debug: msg="long-live token '{{lltoken.stdout}}'"

    - name: Show dashboard run command
      tags: show dashboard run command
      debug: msg="run 'kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 --address=0.0.0.0' for port forwarding to Dashboard"

# ==========================================

- name: Install ingress controller "Ingress-nginx"
  tags: install ingress controller
  hosts: control-1
  become: true
  tasks:
    - name: Setup ingress controller
      tags: setup ingress controller
      command: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.0-beta.0/deploy/static/provider/baremetal/deploy.yaml

    - name: Scale ingress-nginx deployment to 2 replicas
      tags: scale ingress-nginx
      command: kubectl scale deployment ingress-nginx-controller --replicas 2 -n ingress-nginx

    - name: create service for ingress controller
      tags: create service for ingress controller
      command: kubectl apply -f /vagrant/kubernetes/manifests/service-for-ingress-controller.yaml

# ==========================================

- name: Install Longhorn prerequirements
  tags: longhorn prerequirements
  hosts: workers
  become: true
  tasks:
    - name: Install Longhorn prerequirements
      tags: install longhorn prerequirements
      ansible.builtin.apt:
        name:
          - open-iscsi
          - nfs-common
          - cryptsetup

        state: present
        update_cache: yes

    - name: Start iscsid
      tags: enable iscsid
      ansible.builtin.service:
        name: iscsid
        state: started
        enabled: true

    - name: Add the iscsi_tcp module and make sure it is loaded after reboots
      tags: modprobe iscsi_tcp
      community.general.modprobe:
        name: iscsi_tcp
        state: present
        persistent: present

    - name: Add the dm_crypt module and make sure it is loaded after reboots
      tags: modprobe dm_crypt
      community.general.modprobe:
        name: dm_crypt
        state: present
        persistent: present

    - name: Disable multipathd (https://longhorn.io/kb/troubleshooting-volume-with-multipath/)
      tags: disable multipathd
      ansible.builtin.service:
        name: multipathd
        state: stopped
        enabled: false
        masked: true

    - name: Create a volume group on top of /dev/sdb
      community.general.lvg:
        vg: vg-storage
        pvs: /dev/sdb

    - name: Create a logical volume
      community.general.lvol:
        vg: vg-storage
        lv: lv-storage
        size: 100%FREE
        state: present

    - name: Create a ext4 filesystem on logical volume
      community.general.filesystem:
        fstype: ext4
        dev: /dev/vg-storage/lv-storage

    - name: mount the lv on /storage
      tags: mount storage disk
      ansible.posix.mount:
        path: /storage
        src: /dev/vg-storage/lv-storage
        fstype: ext4
        state: mounted

- name: Install Longhorn app
  tags: install Longhorn
  hosts: control-1
  become: true
  tasks:
    - name: Add longhorn helm repo
      kubernetes.core.helm_repository:
        name: longhorn
        repo_url: "https://charts.longhorn.io"

    - name: Deploy longhorn chart using values files
      tags: deploy longhorn
      kubernetes.core.helm:
        name: longhorn
        chart_ref: longhorn/longhorn
        release_namespace: longhorn-system
        create_namespace: true
        values_files:
          - /vagrant/kubernetes/values-longhorn.yaml

    - name: Create ingress rule for longhorn ui
      tags: ingress-longhorn-web
      command: kubectl apply -f /vagrant/kubernetes/manifests/ingress-longhorn.yaml
